{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Telecom Customer Churn - Data Exploration & Modeling\n",
                "\n",
                "This notebook contains the complete pipeline for our Churn Prediction System, built directly from our Python codebase. You can run these cells interactively to explore the dataset, visualize customer segments, and understand the model's explanations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "import shap\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('ggplot')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the raw dataset\n",
                "df = pd.read_csv('../data/raw/customer_churn_dataset.csv')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Customer Segmentation Analysis\n",
                "We previously ran a KMeans clustering algorithm on customer Tenure and Charges to group them into 3 distinct logical segments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_clean = df.copy()\n",
                "df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')\n",
                "df_clean['TotalCharges'].fillna(df_clean['MonthlyCharges'], inplace=True)\n",
                "\n",
                "# Load our trained clustering models\n",
                "kmeans_scaler = joblib.load('../models/kmeans_scaler.pkl')\n",
                "kmeans_model = joblib.load('../models/kmeans_segmentation.pkl')\n",
                "\n",
                "seg_features = df_clean[['tenure', 'MonthlyCharges', 'TotalCharges']]\n",
                "scaled_seg = kmeans_scaler.transform(seg_features)\n",
                "df_clean['Segment'] = kmeans_model.predict(scaled_seg)\n",
                "\n",
                "sns.scatterplot(x='tenure', y='MonthlyCharges', hue='Segment', data=df_clean, palette='viridis')\n",
                "plt.title('Customer Segments by Tenure and Charges')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Explainability directly with SHAP\n",
                "Let's load up our best predictive model (Logistic Regression) and use SHAP to understand which features drive churn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load our classification models\n",
                "best_model = joblib.load('../models/best_model.pkl')\n",
                "# Note: We use shap_background for LinearExplainer reference\n",
                "background_data = joblib.load('../models/shap_background.pkl')\n",
                "features = joblib.load('../models/model_columns.pkl')\n",
                "\n",
                "print(f\"Loaded Model: {type(best_model).__name__}\")\n",
                "\n",
                "# Set up explainer depending on model type\n",
                "if type(best_model).__name__ in ['RandomForestClassifier', 'XGBClassifier', 'XGBRegressor']:\n",
                "    explainer = shap.TreeExplainer(best_model)\n",
                "else:\n",
                "    explainer = shap.LinearExplainer(best_model, background_data)\n",
                "\n",
                "# Calculate SHAP values for a sample\n",
                "sample_to_explain = background_data[:100] # Explain first 100 \n",
                "shap_values = explainer(sample_to_explain)\n",
                "\n",
                "# Plot Global Feature Importance Summary\n",
                "shap.summary_plot(shap_values, sample_to_explain, feature_names=features)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}